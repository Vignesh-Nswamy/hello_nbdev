{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler\n",
    "> Defines methods to crawl all web pages in a specific domain, extract contents from them and store them in a DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "from search_engine.scrapper import parse_webpage\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "def link_filter(link, domain, base_url):\n",
    "    \"\"\"\n",
    "    Filters out links if they... \\n\n",
    "    1. Are not from specified domain \\n\n",
    "    2. Contain extensions - pdf|jpg|jpeg|doc|docx|ppt|pptx|png|txt|exe|ps|psb \\n\n",
    "    3. Contain an `@` \\n\n",
    "    4. Have already been visited\n",
    "    \"\"\"\n",
    "    is_valid = lambda url: not bool(re.search('pdf|jpg|jpeg|doc|docx|ppt|pptx|png|txt|exe|ps|psb|@',\n",
    "                                         url))\n",
    "    return link is not None and is_valid(link) and (link.startswith('/') or domain in link) \\\n",
    "    and urljoin(base_url, link) != base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"link_filter\" class=\"doc_header\"><code>link_filter</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>link_filter</code>(**`link`**, **`domain`**, **`base_url`**)\n",
       "\n",
       "Filters out links if they... \n",
       "\n",
       "1. Are not from specified domain \n",
       "\n",
       "2. Contain extensions - pdf|jpg|jpeg|doc|docx|ppt|pptx|png|txt|exe|ps|psb \n",
       "\n",
       "3. Contain an `@` \n",
       "\n",
       "4. Have already been visited"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(link_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "def link_modifier(url, base_url):\n",
    "    \"\"\"\n",
    "    Converts `relative` urls to absolute ones.\n",
    "    \"\"\"\n",
    "    url = urljoin(base_url, url)\n",
    "    if url[-1]=='/':\n",
    "        url= url[:-1]\n",
    "    if 'https' not in url:\n",
    "        url = url.replace(\"http\", \"https\")\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"link_modifier\" class=\"doc_header\"><code>link_modifier</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>link_modifier</code>(**`url`**, **`base_url`**)\n",
       "\n",
       "Converts `relative` urls to absolute ones."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(link_modifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "def crawl(domain='uic.edu',\n",
    "         url='https://cs.uic.edu',\n",
    "         num_pages=5):\n",
    "    \"\"\"\n",
    "    Starts crawling the specified url and linked urls in a breadth-first fashion,\n",
    "    extracts content and puts them in a DataFrame that will be returned\n",
    "    \"\"\"\n",
    "    # Queue links to crawl\n",
    "    crawl_q = deque([])\n",
    "    # Already crawled links\n",
    "    crawled_links = set([])\n",
    "    # Redundant crawl_q\n",
    "    crawl_q_set = set(crawl_q)\n",
    "    # Number of links crawled\n",
    "    crawl_count = 0\n",
    "    \n",
    "    pages = pd.DataFrame(columns=['id', 'url', 'content', 'graph'])\n",
    "    crawl_q.append(url)\n",
    "    while len(crawl_q) > 0 and crawl_count < num_pages:\n",
    "        try:\n",
    "            crawl_q_set = set(crawl_q)\n",
    "            url = crawl_q.popleft()\n",
    "            if url in crawled_links:\n",
    "                continue\n",
    "            crawled_links.add(url)\n",
    "            content, links = parse_webpage(url)\n",
    "            # Remove invalid links\n",
    "            links = list(filter(lambda link: link_filter(link, domain, url), links))\n",
    "            # Modify relative urls to absolute\n",
    "            links = list(map(lambda link: link_modifier(link, url), links))\n",
    "            # Remove duplicates within the links\n",
    "            links = list(set(links))\n",
    "\n",
    "            pages = pages.append({'id': crawl_count, 'url': url, 'content': content, 'outgoing_links': links}, ignore_index=True)\n",
    "            print(f'Crawled {url}')\n",
    "\n",
    "            # Add links to crawl_q if they are not in crawled links or not already in crawl_q\n",
    "            crawl_q.extend(list(filter(lambda l: l not in crawl_q_set and l not in crawled_links, links)))\n",
    "            crawl_count += 1\n",
    "        except:\n",
    "            print('Error')\n",
    "    \n",
    "    # Clear all lists and queues\n",
    "    crawl_q.clear()\n",
    "    crawled_links = set([])\n",
    "    crawl_q_set = set(crawl_q)\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"crawl\" class=\"doc_header\"><code>crawl</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>crawl</code>(**`domain`**=*`'uic.edu'`*, **`url`**=*`'https://cs.uic.edu'`*, **`num_pages`**=*`5`*)\n",
       "\n",
       "Starts crawling the specified url and linked urls in a breadth-first fashion,\n",
       "extracts content and puts them in a DataFrame that will be returned"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(crawl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
