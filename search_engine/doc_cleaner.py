# AUTOGENERATED! DO NOT EDIT! File to edit: 01_doc_cleaner.ipynb (unless otherwise specified).

__all__ = ['remove_puncts', 'remove_digits', 'stemmer', 'stop_words', 'filter_by_length', 'clean_text', 'clean_tokens',
           'clean']

# Cell
#export
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from string import punctuation, digits

# Cell
#export
remove_puncts = lambda lines: lines.translate(str.maketrans(punctuation,' '*len(punctuation)))
remove_digits = lambda lines: lines.translate(str.maketrans('', '', digits))

stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))
filter_by_length = lambda token: len(token) >= 3 and token not in stop_words

# Cell
#export
def clean_text(doc_lines):
    """
    Removes punctuations and digits from document and returns tokens
    """
    doc_lines = remove_puncts(doc_lines)
    remove_digits = remove_puncts(doc_lines)
    return doc_lines.split()

# Cell
#export
def clean_tokens(tokens):
    """
    Removes stopwords, tokens with length less than 3 characters and stems them
    PorterStemmer is used for stemming
    """
    tokens = [stemmer.stem(t) for t in tokens if t not in stopwords]
    tokens = list(filter(filter_by_length, tokens))
    return tokens

# Cell
#export
def clean(doc):
    """
    Cleans a document by removing punctuations, digits, stopwords, tokens shorter than 3 characters
    """
    tokens = clean_text(doc)
    return clean_tokens(tokens)