# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/00_scrapper.ipynb (unless otherwise specified).

__all__ = ['get_html_content', 'filter_tags', 'get_text_content', 'get_linked_urls', 'parse_webpage']

# Cell
#export
import requests
import re
from bs4 import BeautifulSoup


from bs4.element import Comment

# Cell
#export
def get_html_content(url):
    """
    Gets the raw html content of a given url
    """
    return requests.get(url).content.decode('latin-1')

# Cell
#export
def filter_tags(element):
    """
    Filters out unwanted tags from html text content
    """
    unwanted = {t: '0' for t in ['style', 'script', 'head', 'title', 'meta', '[document]']}
    if element.parent.name in unwanted or isinstance(element, Comment):
        return False
    return True

# Cell
#export
def get_text_content(soup):
    """
    Extracts text from html soup and filters it
    """
    text = soup.findAll(text=True)
    filtered_text = filter(filter_tags, text)
    return u" ".join(t.strip() for t in filtered_text)

# Cell
#export
def get_linked_urls(soup):
    """
    Extracts linked urls from html soup and filters it
    """
    link_cleaner = lambda l: l['href']
    links = soup.findAll('a',href=True)
    return list(map(link_cleaner, links))

# Cell
#export
def parse_webpage(url):
    """
    Extracts usable text and linked urls from given url's html content
    """
    html = get_html_content(url)
    soup = BeautifulSoup(html, 'html.parser')
    return get_text_content(soup), get_linked_urls(soup)