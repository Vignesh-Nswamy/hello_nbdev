# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/03_crawler.ipynb (unless otherwise specified).

__all__ = ['link_filter', 'link_modifier', 'crawl']

# Cell
#export
from .scrapper import parse_webpage
from urllib.parse import urljoin, urlparse

from collections import deque
import pandas as pd
import re

# Cell
#export
def link_filter(link, domain, base_url):
    """
    Filters out links if they... \n
    1. Are not from specified domain \n
    2. Contain extensions - pdf|jpg|jpeg|doc|docx|ppt|pptx|png|txt|exe|ps|psb \n
    3. Contain an `@` \n
    4. Have already been visited
    """
    is_valid = lambda url: not bool(re.search('pdf|jpg|jpeg|doc|docx|ppt|pptx|png|txt|exe|ps|psb|@',
                                         url))
    return link is not None and is_valid(link) and (link.startswith('/') or domain in link) \
    and urljoin(base_url, link) != base_url

# Cell
#export
def link_modifier(url, base_url):
    """
    Converts `relative` urls to absolute ones.
    """
    url = urljoin(base_url, url)
    if url[-1]=='/':
        url= url[:-1]
    if 'https' not in url:
        url = url.replace("http", "https")
    return url

# Cell
#export
def crawl(domain='uic.edu',
         url='https://cs.uic.edu',
         num_pages=5):
    """
    Starts crawling the specified url and linked urls in a breadth-first fashion,
    extracts content and puts them in a DataFrame that will be returned
    """
    # Queue links to crawl
    crawl_q = deque([])
    # Already crawled links
    crawled_links = set([])
    # Redundant crawl_q
    crawl_q_set = set(crawl_q)
    # Number of links crawled
    crawl_count = 0

    pages = pd.DataFrame(columns=['id', 'url', 'content', 'graph'])
    crawl_q.append(url)
    while len(crawl_q) > 0 and crawl_count < num_pages:
        try:
            crawl_q_set = set(crawl_q)
            url = crawl_q.popleft()
            if url in crawled_links:
                continue
            crawled_links.add(url)
            content, links = parse_webpage(url)
            # Remove invalid links
            links = list(filter(lambda link: link_filter(link, domain, url), links))
            # Modify relative urls to absolute
            links = list(map(lambda link: link_modifier(link, url), links))
            # Remove duplicates within the links
            links = list(set(links))

            pages = pages.append({'id': crawl_count, 'url': url, 'content': content, 'outgoing_links': links}, ignore_index=True)
            print(f'Crawled {url}')

            # Add links to crawl_q if they are not in crawled links or not already in crawl_q
            crawl_q.extend(list(filter(lambda l: l not in crawl_q_set and l not in crawled_links, links)))
            crawl_count += 1
        except:
            print('Error')

    # Clear all lists and queues
    crawl_q.clear()
    crawled_links = set([])
    crawl_q_set = set(crawl_q)

    return pages